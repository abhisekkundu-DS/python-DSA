# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rh6F8MCXDdtpHsPLQTpf2lCC1jIPZMl9
"""

import os
os.environ['TH_CPP_MIL_LOG_LEVEL'] = '2'
!pip install pandas
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#Make numpy printouts easier to read
np.set_printoptions(precision=3,suppress=True)

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
dataset_path = tf.keras.utils.get_file("auto-mpg.data", url)

dataset = pd.read_csv(url, names=["MPG", "Cylinders", "Displacement", "Horsepower", "Weight", "Acceleration", "Model Year", "Origin"],na_values='?',comment='\t',sep=' ',skipinitialspace=True)

dataset.head()

#Clead_data_set
dataset = dataset.dropna()
#conver categorical 'origin' data into one -hot data
origin = dataset.pop('Origin')
dataset['USA'] = (origin == 1)*1
dataset['Europe'] = (origin == 2)*2
dataset['Japan'] = (origin == 3)*1

dataset.tail()

# split the data into train and test
train_dataset = dataset.sample(frac = 0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)

print(dataset.shape, train_dataset.shape, test_dataset.shape)
train_dataset.describe().transpose

#split featurs from labels

train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('MPG')
test_labels = test_features.pop('MPG')

def plot(feature ,x=None,y=None):
  plt.figure(figsize=(10,8))
  plt.scatter(train_features[feature],train_labels,label='Data')
  if x is not None and y is not None:
    plt.plot(x,y,color='k', label='Predictions')
  plt.xlabel(feature)
  plt.ylabel('MPG')
  plt.legend()

plot('Horsepower')

plot('Weight')

print(train_dataset.describe().transpose()[['mean','std']])
# Corrected import for Normalization
from tensorflow.keras.layers import Normalization
normalizer = Normalization()
normalizer.adapt(np.array(train_dataset))
print(normalizer.mean.numpy())

first = np.array(train_dataset[:1])
print('First example:',first)
print('Normalized :',normalizer(first).numpy())

#Regression
#1.Normalize the input horsepower
#2. Apply a linear transformation (y = m*x + b)  to produce 1 output using layers.Dense

feature = 'Horsepower'
single_feature = np.array(train_features[feature])
print(single_feature.shape,train_features.shape)

#Normalization
# Use tf.keras.layers.Normalization instead of keras.preprocessing.Normalization
single_feature_normalizer = tf.keras.layers.Normalization(axis=None) # Specify axis=None for a single feature

#adapt to the data
single_feature_normalizer.adapt(single_feature)

#sequential model

single_feature_model = keras.models.Sequential([
    single_feature_normalizer,
    layers.Dense(units = 1)
])



single_feature_model.summary()

#Loss and Optimizer

loss = keras.losses.MeanAbsoluteError()
optim = keras.optimizers.Adam(learning_rate=0.1) # Changed lr to learning_rate

single_feature_model.compile(optimizer=optim,loss=loss)

history = single_feature_model.fit(
    train_features[feature],train_labels,
    epochs = 100,
    verbose = 1,
    validation_split = 0.2

)

def plot_loss(history):
  plt.plot(history.history['loss'],label='loss')
  plt.plot(history.history['val_loss'],label='val_loss')
  plt.ylim([0,25])
  plt.xlabel('Epoch')
  plt.ylabel('Error [ MPG]')
  plt.grid(True)
plot_loss(history)

single_feature_model.evaluate(test_features[feature],test_labels,verbose=1)

#predict and plot
range_min = np.min(single_feature)
range_max = np.max(single_feature)
print(range_min,range_max)
x = tf.linspace(range_min,range_max,100)
y = single_feature_model.predict(x)
plot(feature,x,y)

#DNN
dnn_model = keras.Sequential([
    single_feature_normalizer,
    layers.Dense(64,activation='relu'),
    layers.Dense(64,activation='relu'),
    layers.Dense(1)
])

dnn_model.compile(loss=loss,optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)) # Corrected optimizer access
dnn_model.summary()